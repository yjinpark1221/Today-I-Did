
## ✅ Today I Did

### Compiler Construction

#### 02. Lexical Analysis (1)

슬라이드 29쪽까지 정리 완료

##### Frontend Structure

Source Code(`foo.c`)
**→ Language Preprocessor** (`#include`, `#define`, `#ifdef`, etc)
****→ Preprocessed source code (`foo.i`)
**→ Lexical Analysis, Syntax Analysis, Semantic Analysis**
→ Abstract Syntax Tree

##### Lexical Analysis (== Scanning)

- Specification : how to specify lexical patterns (어휘의)
    - regular expression!
- Recognition : how to recognize the lexical patterns
    - deterministic finite automata
- Automation : how to automatically generate string recognizers from specifications?
    - Thompson's construction and subset construction
- Introduction
    - Tokens
        - keywords : `if`, `while`, … (나열)
        - identifiers : `x`, `y11`, `elsex`, (regular expression)
        - integers, `2`, `1000`, `-20`
        - floating-point : `2.0`, `-.02`, `1e5`
        - special symbols : `+`, `*`, `>=`, …
        - strings : `"x"`, `"He said, \"I luv compiler\""`
    - special case of pattern matching
        - regular expressions : standard notation for representing the patterns
        - finite automata : algorithms for recognizing patterns
    - process : parsing과 섞여있다.
        - multi-character input stream(preprocessed source code) → token stream
        - reduce length of program representation by removing spaces
        - e.g.  `a[index] = 4 + 2` → `a` / `[` / `index` / `]` / `=` / `4` / `+` / `2`

##### Regular Expression

- defined inductively
- a : ordinary character stands for itself
- $\epsilon$ : empty string
- R | S : alteration, **Choice**
    - L(r|s) = L(r) U L(s)
- RS : **concatenation**
    - L(rs) = L(r)L(s)
- R* : concatenation of R, 0 or more times, **Repetition**
    - L(a*) = L(a)*
- R+ : one or more repetitions (RR*)
- . : any symbol in the alphabet
- - : a range of symbols
    - [a-z], [a-zA-Z]
- ~, ^ : any symbol not in a given set
    - ~(a|b|c) == [^abc]
- R? : optional subexpressions
- [abcd] : a|b|c|d
- [^a-z]
- L(R) : language defined by R, set of strings of characters
    - L(abc) = {abc}
    - L( hello | goodbye ) = { hello, goodbye }
    - L(1(0|1)*) = all binary numbers that start with a 1
    - Each token can be define using a regular expression

| R | L(R) |
| --- | --- |
| a | “a” |
| ab | “ab” |
| a | b | “a”, “b” |
| (ab)* | “”, “ab”, “abab”, … |
| (a|$\epsilon$)b | “ab”, “b” |
| digit = [0-9] | “0”, “1”, “2”, … |
| posint = digit+ | “8”, “412”, … |
| int = -?posint | “-23”, “34”, … |
| real = int($\epsilon$|(.posint))
= -?[0-9]+ ($\epsilon$ | .[0-9]+)) | “-1.56”, “12”, “1.056”, …
(”.45”는 포함 안됨) |
| (a|b)c | ac, bc |
| (a|bb)* | $\epsilon$, a, bb, aa, abb, bba, bbbb, … |
| (a|c)*b(a|c)* | set of all strings over {a, b, c} containing exactly one b |
| (a|c)*(b|$\epsilon$)(a|c)* | set of all strings over {a, b, c} containing at most one b |
| (a|c)*(b(a|c))*(a|c)* | no two consecutive b's |
| ((b|c)*a(b}c)*a)*(b|c)* | a가 짝수 개 |
- Definitions
    - symbols : characters
    - alphabet($\Sigma$) : set of legal symbols
    - strings : concatenation of symbols
- Precedence of operations
    - *(repetition) > .(concatenation) > | (choice)
- Regular expressions for PL tokens
    - reserved words
        - *reserved* = if | while } do | …
    - special symbols
        - +, =, :=, ++, …
    - identifiers
        - letter = [a-zA-Z]
        digit = [0-9]
        identifier = letter(letter|digit)*
    - numbers
        - nat = [0-9]+
        - signedNat = (+|-)?nat
        - number = signedNat(”.”nat)?(E signedNat)?
    - comments
        - {Pascal comment} ⇒ { (~})* }
        - — Ada comment ⇒ — (~newline)*
        - /* C comment */ ⇒ a = /, b = *
        ba (b*(a*~(a|b)b*)*a* ) ab
        - handled by ad hoc methods
    - ambiguity ⇒ disambiguating rules
        - keyword >> identifier
        - principle of longest substring
            - e.g., temp 는 te + mp 이냐 temp이냐
    - Token delimiters
        - white space = (blank | tab | newline | comment)+
        - characters that are unambiguously part of other tokens
    - lookahead and backtrack
        - single-character lookahead
        - backtrack (more than single-character lookahead)
    - how to break up text
        - RE's + longest matching token rule +  priorities = definition of a lexer
    
### Automatic Generation of Lexers

- LEX : transducer, transforms an input stream into the alphabet of the grammar processed by YACC
    - flex = fast lex 나중에 만들어짐
- YACC/bison : Yet Another Compiler/Compiler (다음 주)
- inputs : list of RE in priority order
    
    ⇒ Associated action with each RE 
    
- outputs : tokens according to the REs
- Lex Specification : three sections
    - Definition Section
        - `%{ ... }` 안에 있는 코드 → resultant program으로 복사됨
        - token definition established by the parser
        - pattern names for complex patterns by user
        - any additional lexing states
        - pattern and state definitions must start in column 1
            - 공백으로 시작하는 줄 → c 프로그램으로 복사
    - Rules section
        - lexical patterns
        - semantic actions to be performed upon a pattern match
            - `{ ... }` : actions
            - 공백으로 시작하는 줄 → c 프로그램으로 복사
    - User functions section
        - 모든 줄 → c 프로그램으로 복사

// TODO :  LEX 프로그램 코드는 나중에 다시 보기

### FSA

- Finite State Automaton
    - formal basis for lexical analysis
    - used for recognizing patterns represented by REs
    - error transitions are not drawn
    - consists of
        - finite set of states
        - transitions between states
        - an initial state
        - a set of final states == accepting states
- Non-deterministic finite automata (NFA)
    - multiple possible transitions
        
        → state + symbol → next state not unique
        
    - $\epsilon$-transition : some transitions that do not require an input
    - Merging NFAs : $\epsilon$-transition makes merging automata without combining states
- Deterministic finite automata (DFA)
    - transition from each state is uniquely determined by current input character
        
        → state + symbol → next state unique
        
    - Merging DFAs
        - DFA for each token → DFA for some tokens
        - begin with same symbol ⇒ [other] 사용 (lookahead)
- Representation
    - Graph
    - Transition table

// TODO : 71페이지부터 다시 정리

// NOTE : 그래프와 예시가 많으므로 강의자료 보고 다시 복습 필요
### CMU Database Systems

#### 14강 Query Planning & Optimization Part 1.

##### Overview

- SQL is declarative : SQL tells DBMS what answer they want
- DBMS translates it into executable query plan
    - different ways → differences in performance
    - DBMS picks the best plan for a given query ⇒ Optimizer
- Two Optimization Strategies
    - Heuristics/Rules: Rewrite the query to remove inefficiency (cost model X)
    (e.g., WHERE 1 == 0)
    - Cost-based Search : Use **cost model** to evaluate plans and pick the best one
- Architecture Overview
    1. SQL Query
    → SQL Rewriter
    2. SQL Query
    → Parser
    3. Abstract Syntax Tree
    → Binder : Name → Internal Id
    4. Logical Plan
    → Tree Rewriter (Schema info)
    5. Logical Plan
    → Optimizer (Schema Info, estimates by Cost Model) 
    6. Physical Plan
- NP-Hard ⇒ 돈 많이 받음
- Logical Plan : Scan, join, …
    - relational algebra expression to logical plan mapping : mostly 1:1
- Physical Plan : Index scan, Hash join, …
    - logical to physical mapping : not always 1:1

##### Relational Algebra Equivalences (Rule-based Query Optimization)

- Two relational algebra expressions are equivalent
== generate the same set of tuples
- query rewriting: identifying better query without a cost model
    - Predicate Pushdown: predicate filtering before join
    - Projections Pushdown: projection earlier ⇒ copy할 데이터 크기 줄일 수 있음
    - Expression Simplification: rewrite predicate expressions by exploiting transitive properties of boolean logic
        - impossible / unnecessary predicates
        - join elimination
        - ignoring projections
        - merging predicates


#### 15강 Query Planning & Optimization Part 2.

##### Plan Cost Estimation (Cost-based Query Optimization)

- e.g., n-way join → orderings? 4^n ways(Catalan number)
- resources
    - CPU
    - Disk
    - Memory
    - Network
- statistics : internal statistics about tables, attributes, and indexes in its internal catalog
    - $N_R$ : Number of tuples in $R$
    - $V(A, R)$ : number of distinct values for attribute $A$
    - assumptions
        1. Data Uniformity
            
            ⇒ derived statistics 
            $**SC(A, R)$ : selection cardinality**
            
            - average number of records with a value for an attribute $A$ given $N_R/V(A,R)$
            - assumption : **data uniformity**
        2. Independency ⇒ selectivity 이용
        3. Inclusion Principle
            - the domain of join keys overlap, each key in the inner relation → exists in the outer relation
    - Selectivity $sel(P)$ : P라는 predicate을 만족하는 tuple의 비율
        - equality predicate : $sel(A=constant) = SC(P) / N_R$
        - range predicate : $sel(A≥a) = (A_{max} - a) / (A_{max} - A_{min})$
        - negation predicate : $sel(not\ P) = 1 - sel(P)$
        - conjunction & disjunction
- Statistics storage
    - Histograms : use buckets to reduce overhead
    quantiles (variable-length buckets → ANALYZE 할 때 업데이트)
    - Sampling: estimate predicate selectivities (high-end)

##### Plan Enumeration

- Single-relation Query Planning : best access method
    - Sequential Scan
    - Binary Search (clustered indexes)
    - Index Scan
    
    ⇒ heuristics
    
    - OLTP queries are easy to plan because they are **sargable** (**s**earch **arg**ument **able**)
    best index 찾는 것이 어렵지 X, selectivity 높은 걸로!
- Multi-relation Query Planning (JOIN)
    - System R : only consider left-deep join trees (옛날이지만)
    fully pipelined plan 가능성 (temp file을 disk에 안 써도 됨)
    1. enumerate relation orderings
    left-deep tree 중 어느 걸 고를지
    2. enumerate plans for each operator
    3. enumerate access paths for each table
    
    → dp로 계산, 실제로는 더 복잡
    
    - Postgress Optimizer
        1. Traditional Dynamic Programming Approach
        2. Genetic Query Optimizer (GEQO) : relation이 12개 이상이면 이용, 세대 별 유전자 이용

##### Nested Sub-queries

- Approches
    - Rewrite to de-correlated query
    - Decompose nested query and store result in sub-table


## ⏰ 루틴 만들기

기상: 08시 40분

등교: 09시 00분

하교: 23시 10분 => 버근데...

취침: 24시 30분 

## ✔ To-do

- [x] 점심 - 김치찜 11:30
- [x] 저녁 - 학식 등심돈까스 17:30
- [x] 야식 - 치킨 20:00
- [x] 택배 냉장고에 넣어놓기
- [x] 컴파일러설계 수업 듣기
- [x] 컴파일러설계 당일 복습
- [ ] 기계학습이론 수업 듣기
- [x] CMU Databases Systems 14강
- [x] CMU Databases Systems 15강

## 💭 일기

내일 밤까지는 일요일 배드민턴 vs. 아빠 보러 가기 결정하자

오늘 연구실에 대한 이런저런 얘기들을 많이 들었다. 조금 걱정도 되지만 일단은 마냥 재밌다! 처음으로 조금 늦게까지 있어봤는데 시간 가는 줄도 몰랐다. 이런 생활이라면 나쁘지 않지. 민턴 시간만 보장되면... 

## ✈ Tomorrow

- [ ] 기계학습이론 수업 2강 듣기
- [ ] CMU Databases Systems 16강
- [ ] CMU Databases Systems 17강
- [ ] 분산컴퓨팅 수업 & 복습
- [ ] 수치해석 수업 & 복습
