
## ✅ Today I Did
### 수치해석
#### 1. Introduction 정리
##### Numerical Analysis
Solving problems that cannot be solved analytically in a closed form
e.g., nonlinear equations
Calculating special functions such as sin(), cos(), exp(), …
E.g., calculation of sin function using Taylor series
Solving scientific and engineering problems using computers
e.g., analysis of electro-magnetic field, optimization of engineering problem, …
- Objective and Scope
  - how to solve the given problem
  - objective
    - design of numerical algorithms -> computational theory
    - using numerical packages -> ordinary users (scientists/engineers)
    => 이 사이 어딘가
  - scope
    - solving nonlinear eq.
    - solving a large set of linear eqs.
    - maximization/minimization of a function
    - data modeling
    - solving differential eq
    - calculation of DSP
- engineering problem solving
  - precomputed era : Formulation -> **Solution** -> Interpretation 
  - computer era : **Formulation** -> Solution -> **Interpretation**
##### Overview of the topics
- roots and optimization
- linear algebraic equations
- curve fitting
- integration and differentiation
- differential equations
- sorting
---
##### Representation of Numbers
- Finite number of bits for representing a number
- floating point representations
- s * M * B ^ (e - E)
  - s : sign bit
  - M : exact positive integer mantissa
  - B : base (2 or 16)
  - e : exact integer exponent
  - E : bias of the exponent (machine dependent)
- IEEE Binary Floating Point Arithmetic Standard 754-1985
  - Double precision real numbers (64 bits)
  - (-1)^s * 2^(c - 1023) * (1 + f)
    - c : 11 bit exponent
    - f : 52 bit binary fraction
    - s : sign bit
  - smallest number : c = 0x00…0001, f = 0x00…0
    - underflow => set to 0
  - largest number : c = 0x11…1110, f = 0x11…1
    - overflow => stop
- Machine accuracy : the smallest floating-point number which when added to the floating-point number 1.0, produces a floating-point result different from 1.0
  - epsilon = b ^ (1-m), m = # of bits for mantissa
- Accuracy vs. Precision
  - ppt 4분할 그래프
- Roundoff error
  - due to machine accuracy
    - Chopping :  버림
    - Symmetric rounding : 반올림
  - Minimizing roundoff errors
    - keep intermediate values around +1, -1 to avoid overflow or underflow
    - Minimize the number of arithmetic manipulations in order to suppress the accumulation of errors
      - e.g., nested multiplication
    - avoid subtractive cancellation
      - avoid subtraction btw similar numbers
      - start from small numbers
    - use double precision
- Loss of significant digit (유효숫자)
  - e.g., x(sqrt(x+1)-sqrt(x))를 유효숫자 6자리로 구하시오
  - 바로 계산하면 유효숫자 3개로 줄어듦
  - 식 바꿔서 계산하면 유효숫자 6개 + error
- Truncation error
  - due to approximation of formula
  - e.g., Taylor 급수에서 항 줄여서 생김 
  - Approximation
    - zero order, first order, second order …
    - approximation of the 1st derivative
      - forward difference
      - backward difference
      - centered difference
- Error
  - absolute error : E_t = true value - approximation
  - Relative error : e_t = E_t / true value
  - Approximate relative error : e_a = (Approx.TrueValue - approxi) / Approx.TrueValue
    - e_a = (current approximation - previous approximation) / current approximation
  - Stopping condition of iterative algorithm : |e_a| < e_s
    - e_s : desired error
- Data error
  - x˜ : approximation of x
  - ∆x˜: estimate of the error of x
  - data error : ∆ƒ(x˜) = |ƒ(x) -ƒ(x˜)| ≈ |ƒ´(x˜)| ∆x˜
  - data relative error : e_f = (ƒ(x) - ƒ(x˜))/ƒ(x) ≈ ∆ƒ(x˜) / ƒ(x˜)
  - relative error of x : e_x = (x - x˜) / x˜
  - condition number : ratio of these relative errors
    - e_f / e_x = x˜ |ƒ´(x˜)| / ƒ(x˜)
    - condition number >> 1 : ill-conditioned
    - condition number < 1 : error reduction by ƒ
  - error propagation
    - single variable function => ∆ƒ(x˜) ≈ |ƒ´(x˜)| ∆x˜
    - multivariable function => ∆ƒ(x_1˜, x_2˜, … , x_n˜) = (∂ƒ/∂x_1)∆x_1˜ + …
  - total error  = roundoff error + truncation error (tradeoff)
---
#### optional HW

https://github.com/yjinpark1221/2023_MAT3008_NA/commit/f13fa1a81e31f3dacf31e1e5ebb8b5159c4fda84

---
#### 2. Root Finding 1 정리 
f(x) = 0
When there is no analytic solution
### General procedure of root finding
Step 1. Searching rough initial solutions
- graphical method
- incremental search
- experience
- solution of a simplified model
Step 2. Finding an exact solution
- Bracketing method
  - Bisection
  - Linear interpolation
- Open method
  - Newton-Raphson method
##### Bracketing method
- assumption: solution in [a, b]
- key idea: reduce interval systematically
- merit: convergence to an exact solution
- methods
  - bisection method
  - linear interpolation
##### (Bracketing) Bisection method
= Half interval method = Bolzano method
- Principle : 부호 다르면 해가 존재한다. 
- pros
  - simple
- cons
  - slow convergence : /2^n
  - multiple roots
##### (Bracketing) Linear interpolation
= False position method
절반 대신 직선을 그어서 구간을 줄인다
- convergence speed
  - faster than bisection method
  - depends on curvature (sometimes extremely slow)
=> Modified linear interpolation
- 그대로 유지된 끝값이 x_2일 때,  다음에 긋는 직선은 0.5 f(x_2)로 긋는다. =>faster convergence
##### Open method
어떤 점 주변에 답이 있는 경우 
- key idea : starting from an arbitrary point, find the root based on a regular iteration
- risk : divergence
- merit : faster convergence than bracketing methods
- methods
  - fixed-point iteration
    f(x) = 0 => g(x) = x로 고쳐서 p_n+1 = g(p_n)
  - Newton-Raphson method
    - 접선과 x축의 교점으로 이동
    - quadratic convergence (매우 빠름)
    - cons
      - derivative 계산해야함
      - initial guess 영향 큼
      - cycling, wandering, overshooting
  - Secant method
    - 미분 안해도 되는 방법
    - p_(n-1) , p_n => p_(n+1) 계산
    - similar complexity to linear interpolation, faster convergence
    - more stable than Newton Raphson method
    - convergence : |g’(x)| < 1 이면 수렴 == contraction mapping
  - Muller method
    - Generalization of Secant method (2치함수)
- accelerating convergence
  - Aitken’s delta-squared method
    - p_n, n+1, n+2 세 가지로 구하는 q_n도 p로 수렴한다. (더 빠르게)
- fail-safe method : Newton + Bisection 
  - if (Newton 안 되면)bisection
##### Initial step : finding a rough approximation
- Graphical method : 그래프 확대해가면서 직관적으로 찾기
- Incremental Search : 컴퓨터 시켜서 부호 바뀌는 구간 찾기
  - sign change -> solution exists
  - difficulty
    - size of dx => tradeoffs
    - multiple roots -> possibility of solution missing
      - derivatives at both ends


### 분산컴퓨팅 1단원 정리

## ⏰ 

취침: 02시 00분

기상: 09시 55분

등교: 11시 05분

하교: 23시 30분

## ✔ To-do
- [ ] 논문 읽기 - X. Yu, et al., Staring into the Abyss: An Evaluation of Concurrency Control with One Thousand Cores, in VLDB, 2014
- [x] 11:00 교양배드민턴
- [x] 20:00 하이민턴 정기운동
- [x] 수치해석 Introduction 복습
- [x] 수치해석 Root finding 1 복습
- [ ] 수치해석 Root finding 2 복습
- [ ] 수치해석 과제 마무리

## 💭 일기

어째서 이렇게 효율적이지 못하게 공부를 하는 거야

오늘 배드민턴 인생 게임을 쳤다... 우리만의 아시안 게임. 연구실에 간식거리를 둬야겠다.

## ✈ Tomorrow

- [ ] 10:30 피부과
- [ ] 수치해석 Root finding 2 복습
- [ ] 수치해석 Linear equation 1 복습
- [ ] 기계학습이론 시험공부
- [ ] 근로 3주차 번역
