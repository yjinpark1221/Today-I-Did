
## âœ… Today I Did
### ìˆ˜ì¹˜í•´ì„
#### 1. Introduction ì •ë¦¬
##### Numerical Analysis
Solving problems that cannot be solved analytically in a closed form
e.g., nonlinear equations
Calculating special functions such as sin(), cos(), exp(), â€¦
E.g., calculation of sin function using Taylor series
Solving scientific and engineering problems using computers
e.g., analysis of electro-magnetic field, optimization of engineering problem, â€¦
- Objective and Scope
  - how to solve the given problem
  - objective
    - design of numerical algorithms -> computational theory
    - using numerical packages -> ordinary users (scientists/engineers)
    => ì´ ì‚¬ì´ ì–´ë”˜ê°€
  - scope
    - solving nonlinear eq.
    - solving a large set of linear eqs.
    - maximization/minimization of a function
    - data modeling
    - solving differential eq
    - calculation of DSP
- engineering problem solving
  - precomputed era : Formulation -> **Solution** -> Interpretation 
  - computer era : **Formulation** -> Solution -> **Interpretation**
##### Overview of the topics
- roots and optimization
- linear algebraic equations
- curve fitting
- integration and differentiation
- differential equations
- sorting
---
##### Representation of Numbers
- Finite number of bits for representing a number
- floating point representations
- s * M * B ^ (e - E)
  - s : sign bit
  - M : exact positive integer mantissa
  - B : base (2 or 16)
  - e : exact integer exponent
  - E : bias of the exponent (machine dependent)
- IEEE Binary Floating Point Arithmetic Standard 754-1985
  - Double precision real numbers (64 bits)
  - (-1)^s * 2^(c - 1023) * (1 + f)
    - c : 11 bit exponent
    - f : 52 bit binary fraction
    - s : sign bit
  - smallest number : c = 0x00â€¦0001, f = 0x00â€¦0
    - underflow => set to 0
  - largest number : c = 0x11â€¦1110, f = 0x11â€¦1
    - overflow => stop
- Machine accuracy : the smallest floating-point number which when added to the floating-point number 1.0, produces a floating-point result different from 1.0
  - epsilon = b ^ (1-m), m = # of bits for mantissa
- Accuracy vs. Precision
  - ppt 4ë¶„í•  ê·¸ë˜í”„
- Roundoff error
  - due to machine accuracy
    - Chopping :  ë²„ë¦¼
    - Symmetric rounding : ë°˜ì˜¬ë¦¼
  - Minimizing roundoff errors
    - keep intermediate values around +1, -1 to avoid overflow or underflow
    - Minimize the number of arithmetic manipulations in order to suppress the accumulation of errors
      - e.g., nested multiplication
    - avoid subtractive cancellation
      - avoid subtraction btw similar numbers
      - start from small numbers
    - use double precision
- Loss of significant digit (ìœ íš¨ìˆ«ì)
  - e.g., x(sqrt(x+1)-sqrt(x))ë¥¼ ìœ íš¨ìˆ«ì 6ìë¦¬ë¡œ êµ¬í•˜ì‹œì˜¤
  - ë°”ë¡œ ê³„ì‚°í•˜ë©´ ìœ íš¨ìˆ«ì 3ê°œë¡œ ì¤„ì–´ë“¦
  - ì‹ ë°”ê¿”ì„œ ê³„ì‚°í•˜ë©´ ìœ íš¨ìˆ«ì 6ê°œ + error
- Truncation error
  - due to approximation of formula
  - e.g., Taylor ê¸‰ìˆ˜ì—ì„œ í•­ ì¤„ì—¬ì„œ ìƒê¹€ 
  - Approximation
    - zero order, first order, second order â€¦
    - approximation of the 1st derivative
      - forward difference
      - backward difference
      - centered difference
- Error
  - absolute error : E_t = true value - approximation
  - Relative error : e_t = E_t / true value
  - Approximate relative error : e_a = (Approx.TrueValue - approxi) / Approx.TrueValue
    - e_a = (current approximation - previous approximation) / current approximation
  - Stopping condition of iterative algorithm : |e_a| < e_s
    - e_s : desired error
- Data error
  - xËœ : approximation of x
  - âˆ†xËœ: estimate of the error of x
  - data error : âˆ†Æ’(xËœ) = |Æ’(x) -Æ’(xËœ)| â‰ˆ |Æ’Â´(xËœ)| âˆ†xËœ
  - data relative error : e_f = (Æ’(x) - Æ’(xËœ))/Æ’(x) â‰ˆ âˆ†Æ’(xËœ) / Æ’(xËœ)
  - relative error of x : e_x = (x - xËœ) / xËœ
  - condition number : ratio of these relative errors
    - e_f / e_x = xËœ |Æ’Â´(xËœ)| / Æ’(xËœ)
    - condition number >> 1 : ill-conditioned
    - condition number < 1 : error reduction by Æ’
  - error propagation
    - single variable function => âˆ†Æ’(xËœ) â‰ˆ |Æ’Â´(xËœ)| âˆ†xËœ
    - multivariable function => âˆ†Æ’(x_1Ëœ, x_2Ëœ, â€¦ , x_nËœ) = (âˆ‚Æ’/âˆ‚x_1)âˆ†x_1Ëœ + â€¦
  - total error  = roundoff error + truncation error (tradeoff)
---
#### optional HW

https://github.com/yjinpark1221/2023_MAT3008_NA/commit/f13fa1a81e31f3dacf31e1e5ebb8b5159c4fda84

---
#### 2. Root Finding 1 ì •ë¦¬ 
f(x) = 0
When there is no analytic solution
### General procedure of root finding
Step 1. Searching rough initial solutions
- graphical method
- incremental search
- experience
- solution of a simplified model
Step 2. Finding an exact solution
- Bracketing method
  - Bisection
  - Linear interpolation
- Open method
  - Newton-Raphson method
##### Bracketing method
- assumption: solution in [a, b]
- key idea: reduce interval systematically
- merit: convergence to an exact solution
- methods
  - bisection method
  - linear interpolation
##### (Bracketing) Bisection method
= Half interval method = Bolzano method
- Principle : ë¶€í˜¸ ë‹¤ë¥´ë©´ í•´ê°€ ì¡´ì¬í•œë‹¤. 
- pros
  - simple
- cons
  - slow convergence : /2^n
  - multiple roots
##### (Bracketing) Linear interpolation
= False position method
ì ˆë°˜ ëŒ€ì‹  ì§ì„ ì„ ê·¸ì–´ì„œ êµ¬ê°„ì„ ì¤„ì¸ë‹¤
- convergence speed
  - faster than bisection method
  - depends on curvature (sometimes extremely slow)
=> Modified linear interpolation
- ê·¸ëŒ€ë¡œ ìœ ì§€ëœ ëê°’ì´ x_2ì¼ ë•Œ,  ë‹¤ìŒì— ê¸‹ëŠ” ì§ì„ ì€ 0.5 f(x_2)ë¡œ ê¸‹ëŠ”ë‹¤. =>faster convergence
##### Open method
ì–´ë–¤ ì  ì£¼ë³€ì— ë‹µì´ ìˆëŠ” ê²½ìš° 
- key idea : starting from an arbitrary point, find the root based on a regular iteration
- risk : divergence
- merit : faster convergence than bracketing methods
- methods
  - fixed-point iteration
    f(x) = 0 => g(x) = xë¡œ ê³ ì³ì„œ p_n+1 = g(p_n)
  - Newton-Raphson method
    - ì ‘ì„ ê³¼ xì¶•ì˜ êµì ìœ¼ë¡œ ì´ë™
    - quadratic convergence (ë§¤ìš° ë¹ ë¦„)
    - cons
      - derivative ê³„ì‚°í•´ì•¼í•¨
      - initial guess ì˜í–¥ í¼
      - cycling, wandering, overshooting
  - Secant method
    - ë¯¸ë¶„ ì•ˆí•´ë„ ë˜ëŠ” ë°©ë²•
    - p_(n-1) , p_n => p_(n+1) ê³„ì‚°
    - similar complexity to linear interpolation, faster convergence
    - more stable than Newton Raphson method
    - convergence : |gâ€™(x)| < 1 ì´ë©´ ìˆ˜ë ´ == contraction mapping
  - Muller method
    - Generalization of Secant method (2ì¹˜í•¨ìˆ˜)
- accelerating convergence
  - Aitkenâ€™s delta-squared method
    - p_n, n+1, n+2 ì„¸ ê°€ì§€ë¡œ êµ¬í•˜ëŠ” q_në„ pë¡œ ìˆ˜ë ´í•œë‹¤. (ë” ë¹ ë¥´ê²Œ)
- fail-safe method : Newton + Bisection 
  - if (Newton ì•ˆ ë˜ë©´)bisection
##### Initial step : finding a rough approximation
- Graphical method : ê·¸ë˜í”„ í™•ëŒ€í•´ê°€ë©´ì„œ ì§ê´€ì ìœ¼ë¡œ ì°¾ê¸°
- Incremental Search : ì»´í“¨í„° ì‹œì¼œì„œ ë¶€í˜¸ ë°”ë€ŒëŠ” êµ¬ê°„ ì°¾ê¸°
  - sign change -> solution exists
  - difficulty
    - size of dx => tradeoffs
    - multiple roots -> possibility of solution missing
      - derivatives at both ends


### ë¶„ì‚°ì»´í“¨íŒ… 1ë‹¨ì› ì •ë¦¬

## â° 

ì·¨ì¹¨: 02ì‹œ 00ë¶„

ê¸°ìƒ: 09ì‹œ 55ë¶„

ë“±êµ: 11ì‹œ 05ë¶„

í•˜êµ: 23ì‹œ 30ë¶„

## âœ” To-do
- [ ] ë…¼ë¬¸ ì½ê¸° - X. Yu, et al., Staring into the Abyss: An Evaluation of Concurrency Control with One Thousand Cores, in VLDB, 2014
- [x] 11:00 êµì–‘ë°°ë“œë¯¼í„´
- [x] 20:00 í•˜ì´ë¯¼í„´ ì •ê¸°ìš´ë™
- [x] ìˆ˜ì¹˜í•´ì„ Introduction ë³µìŠµ
- [x] ìˆ˜ì¹˜í•´ì„ Root finding 1 ë³µìŠµ
- [ ] ìˆ˜ì¹˜í•´ì„ Root finding 2 ë³µìŠµ
- [ ] ìˆ˜ì¹˜í•´ì„ ê³¼ì œ ë§ˆë¬´ë¦¬

## ğŸ’­ ì¼ê¸°

ì–´ì§¸ì„œ ì´ë ‡ê²Œ íš¨ìœ¨ì ì´ì§€ ëª»í•˜ê²Œ ê³µë¶€ë¥¼ í•˜ëŠ” ê±°ì•¼

ì˜¤ëŠ˜ ë°°ë“œë¯¼í„´ ì¸ìƒ ê²Œì„ì„ ì³¤ë‹¤... ìš°ë¦¬ë§Œì˜ ì•„ì‹œì•ˆ ê²Œì„. ì—°êµ¬ì‹¤ì— ê°„ì‹ê±°ë¦¬ë¥¼ ë‘¬ì•¼ê² ë‹¤.

## âœˆ Tomorrow

- [ ] 10:30 í”¼ë¶€ê³¼
- [ ] ìˆ˜ì¹˜í•´ì„ Root finding 2 ë³µìŠµ
- [ ] ìˆ˜ì¹˜í•´ì„ Linear equation 1 ë³µìŠµ
- [ ] ê¸°ê³„í•™ìŠµì´ë¡  ì‹œí—˜ê³µë¶€
- [ ] ê·¼ë¡œ 3ì£¼ì°¨ ë²ˆì—­
